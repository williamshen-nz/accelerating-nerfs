<!--Thanks ChatGPT for the assistance in generating this -->
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Accelerating NeRFs</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Font -->
    <link href="https://fonts.googleapis.com/css?family=Noto+Sans" rel="stylesheet">

    <!-- Include Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.2/font/bootstrap-icons.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/style.css">
    <link rel="icon" type="image/png" href="assets/mit_logo.svg"/>

    <style>
    </style>
</head>
<body>
<div class="container-fluid main text-center">
    <div class="intro">
        <div class="row justify-content-center">
            <h2 class="title">üèéüí® Accelerating NeRFs: Optimizing Neural Radiance Fields with Specialized Hardware
                Architectures</h2>
        </div>
        <h5>
            <a href="https://shen.nz/" target="_blank">William Shen</a>*<sup>1</sup>,
            <a href="https://wmcclinton.github.io/" target="_blank">Willie McClinton</a>*<sup>1</sup>
        </h5>
        <p><sup>1</sup>MIT CSAIL</p>
        <p class="small-text">* Denotes equal contribution.</p>
        <div class="row justify-content-center" id="paper-buttons">
            <div class="asset-button">
                <a href="assets/paper.pdf" class="btn btn-primary" target="_blank">
                    <i class="bi bi-file-pdf"></i> Paper
                </a>
            </div>
            <div class="asset-button">
                <a href="https://github.com/williamshen-nz/accelerating-nerfs" class="btn btn-primary" target="_blank">
                    <i class="bi bi-code-slash"></i> Code
                </a>
            </div>
            <div class="asset-button">
                <a href="assets/poster.pdf" class="btn btn-primary" target="_blank">
                    <i class="bi bi-file-post"></i> Poster
                </a>
            </div>
        </div>
    </div>
    <hr>
    <div class="subsection text justify-content-center text-left" id="abstract">
        <h4 class="text-center">Abstract</h4>
        <p>
            Neural Radiance Fields (NeRFs) have recently seen an explosion in interest, with significant interest not
            only from computer vision and graphics researchers but also cinematographers, visual effects artists, and
            roboticists.
        </p>
        <p>
            Substantial efforts have been made to improve training and inference speeds, including algorithmic
            improvements such as multiresolution hash tables (Instant NGP) and voxel grids (DVGO), as well as
            architectural improvements such as using thousands of tiny MLPs (KiloNeRF) or extracting traditional 3D
            representations such as polygons from a NeRF (MobileNeRF).
        </p>
        <p>
            Despite this remarkable progress, limited attention has been paid to optimizing the dataflow and
            investigating the hardware acceleration potential of NeRFs, particularly during inference.
            The motivation of our project is to explore this untapped potential and decrease the computational
            requirements of NeRF models.
            We aim to answer the following key questions:
        </p>
        <ol>
            <li>Which components of the NeRF pipeline (e.g. positional encoding, ray sampling, MLP, volumetric
                rendering) are bottlenecks that are amenable to optimization and acceleration?
            </li>
            <li>Can we design hardware acceleration architectures to overcome these bottlenecks and better exploit
                NeRF dataflows?
            </li>
            <li>How does this affect the performance of NeRF for metrics such as rendering quality, computational
                resources, and energy usage?
            </li>
        </ol>
    </div>
    <hr>
    <div class="subsection">
        <h4>Table of Contents</h4>
        <div class="text">
            <a href="#background">NeRF Background</a><br>
            <a href="#sparsity">Exploiting Activation Sparsity</a><br>
            <a href="#volumetric-rendering">Accelerating Volumetric Rendering</a><br>
            <a href="#quantization">Quantization</a>
        </div>
    </div>
    <hr>
    <div class="nerf-videos" id="background">
        <h4>NeRF Background</h4>
        <div style="max-width: 800px; margin: 1.5rem auto;">
            <img src="assets/nerf-architecture.png" class="img-fluid" alt="NeRF Architecture">
        </div>
        Videos rendered from our NeRF implementation using 32-bit floating point precision.
        <div class="row justify-content-center">
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/chair.mp4" autoplay loop muted></video>
                <div class="video-title">Chair</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/drums.mp4" autoplay loop muted></video>
                <div class="video-title">Drums</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/ficus.mp4" autoplay loop muted></video>
                <div class="video-title">Ficus</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/hotdog.mp4" autoplay loop muted></video>
                <div class="video-title">Hotdog</div>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/lego.mp4" autoplay loop muted></video>
                <div class="video-title">Lego</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/materials.mp4" autoplay loop muted></video>
                <div class="video-title">Materials</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/mic.mp4" autoplay loop muted></video>
                <div class="video-title">Mic</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/ship.mp4" autoplay loop muted></video>
                <div class="video-title">Ship</div>
            </div>
        </div>
    </div>
    <hr>
    <div class="subsection text-left" id="sparsity">
        <h4 class="text-center">Exploiting Activation Sparsity</h4>
        <div class="text justify-content-center">
            <p>
                Our NeRF model consists of 12 fully-connected (FC) layers, 10 of which use the ReLU activation function
                hence resulting in sparse activations. This results in a significant number of ineffectual computations,
                which spend unnecessary energy and computation time.
            </p>
            <p>
                We propose to exploit this activation sparsity by using compressed representations of the activations,
                along with gating and skipping to avoid carrying out unnecesary computations. The figure below shows the
                average input activation sparsities of the FC layers of NeRFs trained on the synthetic datasets.
            </p>
            <p>
                We find that the overall average sparsity is 60.3% across the FC layers, excluding fc_1 and fc_11. Note
                that fc_1 receives the position-encoded ray samples while fc_11 receives the output from fc_10 which
                does not have an activation function.
            </p>
        </div>
        <div id="activationSparsityCrop">
            <img src="assets/activation-sparsity-crop.png" alt="Activation Sparsity (Cropped)" class="img-fluid">
        </div>
        <div class="d-none" id="activationSparsity">
            <img src="assets/activation-sparsity.png" alt="Activation Sparsity" class="img-fluid">
        </div>
        <div class="row text-center justify-content-center" style="margin: 1.5rem auto;">
            <div class="col-sm-12">
                <button class="btn btn-secondary" id="activationSparsityBtn">Show More üìä</button>
            </div>
        </div>
        <div class="text justify-content-center">
            <p>
                We use the Eyeriss architecture to accelerate the NeRF model. We gain a significant reduction in energy
                and cycles by exploiting the activation sparsity. The table below shows the results of our experiments.
            </p>
            <!-- TODO: image of table -->
        </div>
    </div>
    <hr>
    <div class="subsection" id="volumetric-rendering">
        <h4 class="text-center">Accelerating Volumetric Rendering</h4>
        <p>TODO: Willie</p>
    </div>
    <hr>
    <div class="subsection" id="quantization">
        <h4 class="text-center">Quantization</h4>
        <div class="text text-left justify-content-center">
            <p>
                <strong>Question:</strong> The NeRF models by default use 32-bit floating point numbers (FP32). Can
                we quantize the model to FP16 to speed up inference, and what tradeoffs do we make as a result?
            </p>
            <p>
                <strong>Answer:</strong> Yes, we can quantize the model to fp16 and achieve significant speedups (>2x
                üèéüí® improvement in rendering time) at the cost of decreased peak signal-to-noise ratio (PSNR). While the
                PSNR is decreased, it is difficult to observe any visible difference between the images
                rendered from the FP32 and FP16 models. Thus, FP16 could be sufficient for many applications and
                additionally comes at the benefit of 2x smaller model size.
            </p>
            <p class="small-text" style="color: #a1a1a1">
                Note: we run FP32 and FP16 using PyTorch on a NVIDIA RTX 3090 GPU to determine the render time.
                We estimate energy consumption by multiplying the render time by the average power consumption of the
                GPU.
                <!-- FP32 peak 343W, FP16 peak 326W -->
            </p>
        </div>
        <div class="nerf-videos">
            <div class="row justify-content-center">
                <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                    <h5>Lego (FP32)</h5>
                    <video src="assets/lego.mp4" autoplay loop muted></video>
                    <p>
                        PSNR (avg) = 33.72 dB<br>
                        <span style="color:red">üêå Render Time = 160.85s</span><br>
                        <span style="color:red">Model Size = 2.39 MB</span>
                    </p>
                </div>
                <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                    <h5>Lego (FP16)</h5>
                    <video src="assets/lego_fp16.mp4" autoplay loop muted></video>
                    <p>
                        PSNR (avg) = 32.74 dB<br>
                        <span style="color:green">üèéüí® Render Time = 60.74s</span><br>
                        <span style="color:green">Model Size =  1.20 MB</span>
                    </p>
                </div>
            </div>
            <div class="d-none" id="hiddenVideos">
                <div class="row justify-content-center">
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ficus (FP32)</h5>
                        <video src="assets/ficus.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 32.93 dB<br>
                            <span style="color:red">üêå Render Time = 65.95s</span><br>
                            <span style="color:red">Model Size = 2.39 MB</span>
                        </p>
                    </div>
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ficus (FP16)</h5>
                        <video src="assets/ficus_fp16.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 32.07 dB<br>
                            <span style="color:green">üèéüí® Render Time = 32.33s</span><br>
                            <span style="color:green">Model Size =  1.20 MB</span>
                        </p>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ship (FP32)</h5>
                        <video src="assets/ship.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 29.85 dB<br>
                            <span style="color:red"> üêå Render Time = 368.00s</span><br>
                            <span style="color:red">Model Size = 2.39 MB</span>
                        </p>
                    </div>
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ship (FP16)</h5>
                        <video src="assets/ship_fp16.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 29.43 dB<br>
                            <span style="color:green">üèéüí® Render Time = 128.16s</span><br>
                            <span style="color:green">Model Size =  1.20 MB</span>
                        </p>
                    </div>
                </div>
            </div>
            <div class="row justify-content-center">
                <div class="col-sm-12">
                    <button class="btn btn-secondary" id="quantVideosBtn">Show More üé•</button>
                </div>
            </div>
        </div>
    </div>
    <hr>
    <p class="small-text">
        Course project for
        <a href="http://csg.csail.mit.edu/6.5930/index.html" target="_blank">
            6.5930 Hardware Architecture for Deep Learning - Spring 2023
        </a>
    </p>
</div>

<script>
    // Show activation sparsity
    const activationSparsityBtn = document.getElementById("activationSparsityBtn");
    const activationSparsity = document.getElementById("activationSparsity");
    const activationSparsityCrop = document.getElementById("activationSparsityCrop");

    activationSparsityBtn.addEventListener("click", () => {
        activationSparsity.classList.toggle("d-none");
        activationSparsityCrop.classList.toggle("d-none");
        if (activationSparsityBtn.textContent === "Show More üìä") {
            activationSparsityBtn.textContent = "Hide üìä";
        } else {
            activationSparsityBtn.textContent = "Show More üìä";
        }
    });

    // Toggle hidden videos
    const quantVideosBtn = document.getElementById("quantVideosBtn");
    const hiddenVideos = document.getElementById("hiddenVideos");

    quantVideosBtn.addEventListener("click", () => {
        hiddenVideos.classList.toggle("d-none");
        if (quantVideosBtn.textContent === "Show More üé•") {
            quantVideosBtn.textContent = "Hide üé•";
        } else {
            quantVideosBtn.textContent = "See More üé•";
        }
    });
</script>
</body>
</html>