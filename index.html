<!--Thanks ChatGPT for the assistance in generating this -->
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Accelerating NeRFs</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Font -->
    <link href="https://fonts.googleapis.com/css?family=Noto+Sans" rel="stylesheet">

    <!-- Include Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
          integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.2/font/bootstrap-icons.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="assets/style.css">
    <link rel="icon" type="image/png" href="assets/mit_logo.svg"/>

    <style>
    </style>
</head>
<body>
<div class="container-fluid main text-center">
    <div class="intro">
        <div class="row justify-content-center">
            <h2 class="title">üèéüí® Accelerating NeRFs: Optimizing Neural Radiance Fields with Specialized Hardware
                Architectures</h2>
        </div>
        <h5>
            <a href="https://shen.nz/" target="_blank">William Shen</a>*<sup>1</sup>,
            <a href="https://wmcclinton.github.io/" target="_blank">Willie McClinton</a>*<sup>1</sup>
        </h5>
        <p><sup>1</sup>MIT CSAIL</p>
        <p class="small-text">* Denotes equal contribution.</p>
        <div class="row justify-content-center" id="paper-buttons">
            <div class="asset-button">
                <a href="assets/paper.pdf" class="btn btn-primary" target="_blank">
                    <i class="bi bi-file-pdf"></i> Paper
                </a>
            </div>
            <div class="asset-button">
                <a href="https://github.com/williamshen-nz/accelerating-nerfs" class="btn btn-primary" target="_blank">
                    <i class="bi bi-code-slash"></i> Code
                </a>
            </div>
            <div class="asset-button">
                <a href="assets/poster.pdf" class="btn btn-primary" target="_blank">
                    <i class="bi bi-file-post"></i> Poster
                </a>
            </div>
        </div>
    </div>
    <hr>
    <div class="subsection text justify-content-center text-left" id="abstract">
        <h4 class="text-center">Abstract</h4>
        <p>
            Neural Radiance Fields (NeRFs) have recently seen an explosion in interest, with significant interest not
            only from computer vision and graphics researchers but also cinematographers, visual effects artists, and
            roboticists.
        </p>
        <p>
            Substantial efforts have been made to improve training and inference speeds, including algorithmic
            improvements such as multiresolution hash tables (Instant NGP) and voxel grids (DVGO), as well as
            architectural improvements such as using thousands of tiny MLPs (KiloNeRF) or extracting traditional 3D
            representations such as polygons from a NeRF (MobileNeRF).
        </p>
        <p>
            Despite this remarkable progress, limited attention has been paid to optimizing the dataflow and
            investigating the hardware acceleration potential of NeRFs, particularly during inference.
            The motivation of our project is to explore this untapped potential and decrease the computational
            requirements of NeRF models.
            We aim to answer the following key questions:
        </p>
        <ol>
            <li>Which components of the NeRF pipeline (e.g. positional encoding, ray sampling, MLP, volumetric
                rendering) are bottlenecks that are amenable to optimization and acceleration?
            </li>
            <li>Can we design hardware acceleration architectures to overcome these bottlenecks and better exploit
                NeRF dataflows?
            </li>
            <li>How does this affect the performance of NeRF for metrics such as rendering quality, computational
                resources, and energy usage?
            </li>
        </ol>
    </div>
    <hr>
    <div class="subsection">
        <h4>Table of Contents</h4>
        <div class="text">
            <a href="#videos">Videos</a><br>
            <a href="#sparsity">Activation Sparsity</a><br>
            <a href="#quantization">Quantization</a>
        </div>
    </div>
    <hr>
    <div class="nerf-videos" id="videos">
        <h4>Videos</h4>
        Rendered from our NeRF implementation using 32-bit floating point precision.
        <div class="row justify-content-center">
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/chair.mp4" autoplay loop muted></video>
                <div class="video-title">Chair</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/drums.mp4" autoplay loop muted></video>
                <div class="video-title">Drums</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/ficus.mp4" autoplay loop muted></video>
                <div class="video-title">Ficus</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/hotdog.mp4" autoplay loop muted></video>
                <div class="video-title">Hotdog</div>
            </div>
        </div>
        <div class="row justify-content-center">
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/lego.mp4" autoplay loop muted></video>
                <div class="video-title">Lego</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/materials.mp4" autoplay loop muted></video>
                <div class="video-title">Materials</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/mic.mp4" autoplay loop muted></video>
                <div class="video-title">Mic</div>
            </div>
            <div class="col-sm-2 col-md-2 video-container">
                <video src="assets/ship.mp4" autoplay loop muted></video>
                <div class="video-title">Ship</div>
            </div>
        </div>
    </div>
    <hr>
    <div class="subsection text-left" id="sparsity">
        <h4 class="text-center">Activation Sparsity</h4>
        <div class="text justify-content-center">
            <p>
                We depict the average and standard deviation of the input activation sparsities of the fully-connected
                (FC) layers of NeRFs trained on the synthetic datasets.

                Note that all FC layers use the ReLU activation function except fc_10, which does not have an
                activation function.
            </p>
            <p>
                We find the overall average sparsity is 60.3% across the FC layers, excluding fc_1 and fc_11.
                Note that fc_1 receives the position-encoded ray samples while fc_11 receives the output from fc_10.
            </p>
        </div>
        <img src="assets/activation-sparsity.png" alt="Activation Sparsity" class="img-fluid">
    </div>
    <hr>
    <div class="subsection" id="quantization">
        <h4 class="text-center">Quantization</h4>
        <div class="text text-left justify-content-center">
            <p>
                <strong>Question:</strong> The NeRF models by default use 32-bit floating point numbers (FP32). Can
                we quantize the model to FP16 to speed up inference, and what tradeoffs do we make as a result?
            </p>
            <p>
                <strong>Answer:</strong> Yes, we can quantize the model to fp16 and achieve significant speedups (>2x
                üèéüí® improvement in rendering time) at the cost of decreased peak signal-to-noise ratio (PSNR). While the
                PSNR is decreased, it is difficult to observe any visible difference between the images
                rendered from the FP32 and FP16 models. Thus, FP16 could be sufficient for many applications and
                additionally comes at the benefit of 2x smaller model size.
            </p>
            <p class="small-text" style="color: #b4b4b4">
                Note: we run FP32 and FP16 using PyTorch on a NVIDIA RTX 3090 GPU to determine the render time.
            </p>
        </div>
        <div class="nerf-videos">
            <div class="row justify-content-center">
                <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                    <h5>Lego (FP32)</h5>
                    <video src="assets/lego.mp4" autoplay loop muted></video>
                    <p>
                        PSNR (avg) = 33.72 dB<br>
                        <span style="color:red">üêå Render Time = 160.85s</span><br>
                        <span style="color:red">Model Size = 2.39 MB</span>
                    </p>
                </div>
                <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                    <h5>Lego (FP16)</h5>
                    <video src="assets/lego_fp16.mp4" autoplay loop muted></video>
                    <p>
                        PSNR (avg) = 32.74 dB<br>
                        <span style="color:green">üèéüí® Render Time = 60.74s</span><br>
                        <span style="color:green">Model Size =  1.20 MB</span>
                    </p>
                </div>
            </div>
            <div class="d-none" id="hiddenVideos">
                <div class="row justify-content-center">
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ficus (FP32)</h5>
                        <video src="assets/ficus.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 32.93 dB<br>
                            <span style="color:red">üêå Render Time = 65.95s</span><br>
                            <span style="color:red">Model Size = 2.39 MB</span>
                        </p>
                    </div>
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ficus (FP16)</h5>
                        <video src="assets/ficus_fp16.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 32.07 dB<br>
                            <span style="color:green">üèéüí® Render Time = 32.33s</span><br>
                            <span style="color:green">Model Size =  1.20 MB</span>
                        </p>
                    </div>
                </div>
                <div class="row justify-content-center">
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ship (FP32)</h5>
                        <video src="assets/ship.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 29.85 dB<br>
                            <span style="color:red"> üêå Render Time = 368.00s</span><br>
                            <span style="color:red">Model Size = 2.39 MB</span>
                        </p>
                    </div>
                    <div class="col-sm-3 col-md-3 col-lg-3 video-container">
                        <h5>Ship (FP16)</h5>
                        <video src="assets/ship_fp16.mp4" autoplay loop muted></video>
                        <p>
                            PSNR (avg) = 29.43 dB<br>
                            <span style="color:green">üèéüí® Render Time = 128.16s</span><br>
                            <span style="color:green">Model Size =  1.20 MB</span>
                        </p>
                    </div>
                </div>
            </div>
            <div class="row justify-content-center">
                <div class="col-sm-12">
                    <button class="btn btn-primary" id="seeMoreBtn">Show More üé•</button>
                </div>
            </div>
        </div>
    </div>
    <hr>
    <p class="small-text">
        Course project for
        <a href="http://csg.csail.mit.edu/6.5930/index.html" target="_blank">
            6.5930 Hardware Architecture for Deep Learning - Spring 2023
        </a>
    </p>
</div>

<script>
    const seeMoreBtn = document.getElementById("seeMoreBtn");
    const hiddenVideos = document.getElementById("hiddenVideos");

    seeMoreBtn.addEventListener("click", () => {
        hiddenVideos.classList.toggle("d-none");
        if (seeMoreBtn.textContent === "Show More üé•") {
            seeMoreBtn.textContent = "Hide üé•";
        } else {
            seeMoreBtn.textContent = "See More üé•";
        }
    });
</script>
</body>
</html>